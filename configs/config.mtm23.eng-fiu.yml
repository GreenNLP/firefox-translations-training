####
# Example of a production config
# Change language pair, experiment name, datasets and other settings if needed
# Training low resource languages might require more tuning of pipeline/training/configs
###

experiment:
  dirname: eng-fiu
  name: mtm23
  src: en
  trg: et
  src_three_letter: eng
  trg_three_letter: est
  langpairs:
    - en-fi
    - en-et

  #URL to the OPUS-MT model to use as the teacher
  opusmt-teacher: "https://object.pouta.csc.fi/Tatoeba-MT-models/eng-fiu/opus2m-2020-08-01.zip"
  #URL to the OPUS-MT model to use as the backward model
  opusmt-backward: "https://object.pouta.csc.fi/Tatoeba-MT-models/est-eng/opus+bt-2021-04-30.zip" # this needs to be

  parallel-max-sentences: 100000
  split-length: 10000

  best-model: perplexity

#marian-args: # not working
#  decoding-backward:
#    mini-batch-words: 2000
#  decoding-teacher:
#    mini-batch: 8
#    beam-size: 1

#TODO: extract this info straight from the OPUS model yml info file
datasets:
#  train:
#    - tc_Tatoeba-Challenge-v2021-08-07
  train:
    - opus_ELRC_2922/v1
#    - tc_Tatoeba-Challenge-v2021-08-07
  # datasets to merge for validation while training. Translations by teacher
  devtest:
    - flores_dev
  # datasets for evaluation
  test:
    - flores_devtest
