#OPUS-CAT Terms, eng-fin

experiment:
  name: opuscat-rat-eng-fin
  src: en
  trg: fi


  opusmt-teacher: 'https://object.pouta.csc.fi/Tatoeba-MT-models/eng-fin/opusTCv20210807+bt-2021-09-01.zip'

  teacher-ensemble: 1
  # path to a pretrained backward model (optional)
  backward-model: ""
  # path to a pretrained vocabulary (optional)
  vocab: ""

  rat:
    min-scores: [0.5]
    min-fuzzies: [1]
    max-fuzzies: [1]
    finetune-teacher-with-rat: True

  # Since we are just doing fine-tuning, just use 5 mil sentences
  parallel-max-sentences: 10000000

  # limits per downloaded dataset
  mono-max-sentences-src: 100000000
  mono-max-sentences-trg: 20000000
  # split corpus to parallelize translation
  split-length: 1000000
  # vocab training sample
  spm-sample-size: 1000000

  best-model: chrf

marian-args:
# these configs override pipeline/train/configs
  finetune-teacher-with-terms:
    after: 1e


datasets:
  # parallel training corpus
  tc_scored: ../data/finetuning_data/en-fi/tc_Tatoeba-Challenge-v2023-09-26.scored.gz
  # datasets to merge for validation while training
  devtest:
    - flores_dev
  # datasets for evaluation
  test:
    - flores_devtest
    - sacrebleu_wmt15
    - sacrebleu_wmt16
    - sacrebleu_wmt17
    - sacrebleu_wmt18
    - sacrebleu_wmt19
