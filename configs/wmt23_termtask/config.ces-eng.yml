#WMT23 term task experiment, ces-eng

experiment:
  name: wmt23-termtask-ces-eng
  src: cs
  trg: en

  teacher-ensemble: 1
  # path to a pretrained backward model (optional)
  backward-model: ""
  # path to a pretrained vocabulary (optional)
  vocab: ""

  wmt23_termtask:
    annotation-schemes: ['surface-nonfac-int-append']
    term-ratios: [2]
    sents-per-term-sents: [10]
    train-term-teacher: True
    finetune-teacher-with-terms: True

  # limits per downloaded dataset
  mono-max-sentences-src: 100000000
  mono-max-sentences-trg: 20000000
  # split corpus to parallelize translation
  split-length: 1000000
  # vocab training sample
  spm-sample-size: 10000000

  best-model: chrf

  bicleaner:
    default-threshold: 0.5
    dataset-thresholds:
      mtdata_Statmt-europarl-10-ces-eng: 0
      mtdata_ParaCrawl-paracrawl-9-eng-ces: 0.8
      mtdata_Statmt-commoncrawl_wmt13-1-ces-eng: 0.8
      mtdata_Statmt-news_commentary-16-ces-eng: 0
      mtdata_Statmt-wikititles-3-ces-eng: 0.3
      mtdata_Facebook-wikimatrix-1-ces-eng: 0.3
      mtdata_Tilde-eesc-2017-ces-eng: 0.2
      mtdata_Tilde-ema-2016-ces-eng: 0.2
      mtdata_Tilde-ecb-2017-ces-eng: 0.2
      mtdata_Tilde-rapid-2019-ces-eng: 0.2
    
marian-args:
# these configs override pipeline/train/configs
  training-backward:
    # change based on available training data
    after: 10e
  training-teacher-base:
    early-stopping: 10
    #student training has a very flat increase in metrics, so early stopping  
  ##takes too much time
  training-term-teacher:
    after: 50e
  training-student:
    after: 100e
# these configs override pipeline/translate/decoder.yml
  decoding-backward:
    # 12 Gb GPU, s2s model
    mini-batch-words: 2000
    beam-size: 12
  decoding-teacher:
    # 12 Gb GPU, ensemble of 2 teachers
    mini-batch-words: 1000
    # 2080ti or newer
    precision: float16


datasets:
  # parallel training corpus
  train:
    - mtdata_Statmt-europarl-10-ces-eng
    - mtdata_ParaCrawl-paracrawl-9-eng-ces
    - mtdata_Statmt-commoncrawl_wmt13-1-ces-eng
    - mtdata_Statmt-news_commentary-16-ces-eng
    - mtdata_Statmt-wikititles-3-ces-eng
    - mtdata_Facebook-wikimatrix-1-ces-eng
    - mtdata_Tilde-eesc-2017-ces-eng
    - mtdata_Tilde-ema-2016-ces-eng
    - mtdata_Tilde-ecb-2017-ces-eng
    - mtdata_Tilde-rapid-2019-ces-eng
  # datasets to merge for validation while training
  devtest:
    - flores_dev
    - sacrebleu_wmt17
    - sacrebleu_wmt15
    - sacrebleu_wmt14
  # datasets for evaluation
  test:
    - flores_devtest
    - sacrebleu_wmt20
    - sacrebleu_wmt18
    - sacrebleu_wmt16
    - sacrebleu_wmt13
  # mtdata does not support loading of mono data except as part of recipes, so skip these.
  # In any case, the emphasis here is on terminology, not overall MT quality
  # monolingual datasets (ex. paracrawl-mono_paracrawl8, commoncrawl_wmt16, news-crawl_news.2020)
  # to be translated by the teacher model
  #  mono-src:
  #    - mtdata_Statmt-news_crawl-2021-ces
  #    - mtdata_Statmt-europarl-10-ces
  #    - mtdata_Statmt-news_commentary-17-ces
  #    - mtdata_Statmt-commoncrawl-wmt22-ces
  #    - mtdata_Leipzig-news-2022_1m-ces 
  #    - mtdata_Leipzig-newscrawl-2019_1m-ces
  #    - mtdata_Leipzig-wikipedia-2021_1m-ces
  #    - mtdata_Leipzig-web_public-2019_1m-ces_CZ
  #  # to be translated by the backward model to augment teacher corpus with back-translations
  #  # leave empty to skip augmentation step (high resource languages)
  #  mono-trg:
  #    - mtdata_Statmt-news_crawl-2021-eng
  #    - mtdata_Statmt-news_discussions-2019-eng
  #    - mtdata_Statmt-news_commentary-17-eng
  #    - mtdata_Statmt-commoncrawl-wmt22-eng
  #    - mtdata_Statmt-europarl-10-eng
  #    - mtdata_Leipzig-news-2020_1m-eng
  #    - mtdata_Leipzig-ukweb_public-2018_1m-eng
  #      tatmt-news_commentary-17-eng
